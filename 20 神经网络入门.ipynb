{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 人工智能元年是哪一年？\n",
    "\n",
    "1956年，达特茅斯学院。\n",
    "\n",
    "约翰·麦卡锡：1971年图灵奖\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98yxggh6qj30d80ibdhq.jpg)\n",
    "\n",
    "马文·明斯基：1969年图灵奖\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98yxvmdw1j30dw0j9gnb.jpg)\n",
    "\n",
    "内森尼尔·罗切斯特：IBM第一台商用计算机架构师\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98yzq67svj305c07a749.jpg)\n",
    "\n",
    "克劳德·香农：信息论之父\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98z0blow1j30hs0dcwez.jpg)\n",
    "\n",
    "## 更多的时间节点\n",
    "\n",
    "- 1936年，图灵机诞生。\n",
    "\n",
    "- 1943年，发明感知机。\n",
    "\n",
    "- 1945年，冯诺依曼结构计算机诞生。\n",
    "\n",
    "- 1956年，提出人工智能。\n",
    "\n",
    "（人工智能的第一次低谷）\n",
    "\n",
    "- 1986年，反向传播算法。\n",
    "\n",
    "（人工智能的第二次低谷）\n",
    "\n",
    "- 2012年，AlexNet夺冠，第三次人工智能浪潮。\n",
    "\n",
    "- 2018年，BERT诞生。\n",
    "\n",
    "\n",
    "## 生物神经元\n",
    "\n",
    "单个生物神经元\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98x48m1y1j31400pxwib.jpg)\n",
    "\n",
    "人类大脑皮质的多层神经元网络\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98x4vcf4bj30kn06mju1.jpg)\n",
    "\n",
    "1943年，McCulloch和Pitts提出了一个非常简单的生物神经元模型，它后来演化成了人工神经元。\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98xij2a2yj313z0diwgy.jpg)\n",
    "\n",
    "这些网络的逻辑计算如下：\n",
    "\n",
    "- 左边第一个网络是确认函数：如果神经元 A 被激活，那么神经元 C 也被激活（因为它接收来自神经元 A 的两个输入信号），但是如果神经元 A 关闭，那么神经元 C 也关闭。\n",
    "\n",
    "- 第二个网络执行逻辑 AND：神经元 C 只有在激活神经元 A 和 B（单个输入信号不足以激活神经元 C）时才被激活。\n",
    "\n",
    "- 第三个网络执行逻辑 OR：如果神经元 A 或神经元 B 被激活（或两者），神经元 C 被激活。\n",
    "\n",
    "- 最后，如果我们假设输入连接可以抑制神经元的活动（生物神经元是这样的情况），那么第四个网络计算一个稍微复杂的逻辑命题：如果神经元 B 关闭，只有当神经元A是激活的，神经元 C 才被激活。如果神经元 A 始终是激活的，那么你得到一个逻辑 NOT：神经元 C 在神经元 B 关闭时是激活的，反之亦然。\n",
    "\n",
    "## 人工智能的两次寒冬和两次复兴\n",
    "\n",
    "ANN的早期成功让人们广泛相信，人类马上就能造出真正的智能机器了。1960年代，当这个想法落空时，资助神经网络的钱锐减，ANN进入了寒冬。1980年代早期，诞生了新的神经网络架构和新的训练方法，连结主义（研究神经网络）复苏，但是进展很慢。到了1990年代，出现了一批强大的机器学习方法，比如支持向量机。这些新方法的结果更优，也比ANN具有更扎实的理论基础，神经网络研究又一次进入寒冬。我们正在经历的是第三次神经网络浪潮。\n",
    "\n",
    "### 深度学习“三驾马车”\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g9equ43ecfj30go0bkt94.jpg)\n",
    "\n",
    "从左至右：杨·乐坤、乔弗里·辛顿、约书亚·本桥、吴恩达\n",
    "\n",
    "## 感知机\n",
    "\n",
    "阈值逻辑单元（TLU），或称为线性阈值单元（LTU）\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98xl6smysj30ux0h1jtl.jpg)\n",
    "\n",
    "z = W1x1 + W2x2 + ... + Wnxn = xT·W\n",
    "\n",
    "hW(x) = step(z)，step()是阶跃函数\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98xmjtybbj30xc05qaag.jpg)\n",
    "\n",
    "一个具有两个输入神经元、一个偏置神经元和三个输出神经元的感知机架构\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98xogpsjhj312a0m0whl.jpg)\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98xp0625sj30gu034jre.jpg)\n",
    "\n",
    "在这个公式中，\n",
    "\n",
    "- X表示输入特征矩阵，每行是一个实例，每列是一个特征；\n",
    "\n",
    "- 权重矩阵W包含所有的连接权重，除了偏置神经元。每有一个输入神经元权重矩阵就有一行，神经层每有一个神经元权重矩阵就有一列；\n",
    "\n",
    "- 偏置矢量b含有所有偏置神经元和人工神经元的连接权重。每有一个人工神经元就对应一个偏置项；\n",
    "\n",
    "- 函数phi被称为激活函数，当人工神经网络是TLU时，激活函数是阶跃函数。\n",
    "\n",
    "\n",
    "## 多层感知机MLP\n",
    "\n",
    "MLP 由一个输入层、一个或多个称为隐藏层的 TLU 组成，一个 TLU 层称为输出层。靠近输入层的层，通常被称为浅层，靠近输出层的层通常被称为上层。除了输出层，每一层都有一个偏置神经元，并且全连接到下一层。\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98xymoh7ej30xc0nq0w6.jpg)\n",
    "\n",
    "当人工神经网络有多个隐含层时，称为深度神经网络（DNN）\n",
    "\n",
    "\n",
    "## 反向传播算法\n",
    "\n",
    "1986年，David Rumelhart、Geoffrey Hinton、Ronald Williams 发表了一篇突破性的论文（ https://scholar.google.com/scholar?q=Learning+Internal+Representations+by+Error+Propagation+author%3Arumelhart ），提出了至今仍在使用的反向传播训练算法。总而言之，反向传播算法是使用了高效梯度计算的梯度下降算法：只需要两次网络传播（一次向前，一次向后），就可以算出网络误差的、和每个独立模型参数相关的梯度。换句话说，反向传播算法为了减小误差，可以算出每个连接权重和每个偏置项的调整量。当得到梯度之后，就做一次常规的梯度下降，不断重复这个过程，直到网络得到收敛解。\n",
    "\n",
    "对BP做详细分解：\n",
    "\n",
    "- 每次处理一个微批次（假如每个批次包含32个实例），用训练集多次训练BP，每次被称为一个周期（epoch）；\n",
    "\n",
    "- 每个微批次先进入输入层，输入层再将其发到第一个隐藏层。计算得到该层所有神经元的（微批次的每个实例的）输出。输出接着传到下一层，直到得到输出层的输出。这个过程就是前向传播：就像做预测一样，只是保存了每个中间结果，中间结果要用于反向传播；\n",
    "\n",
    "- 然后计算输出误差（使用损失函数比较目标值和实际输出值，然后返回误差）；\n",
    "\n",
    "- 接着，计算每个输出连接对误差的贡献量。这是通过链式法则（就是对多个变量做微分的方法）实现的；\n",
    "\n",
    "- 然后还是使用链式法则，计算最后一个隐藏层的每个连接对误差的贡献，这个过程不断向后传播，直到到达输入层。\n",
    "\n",
    "- 最后，BP算法做一次梯度下降步骤，用刚刚计算的误差梯度调整所有连接权重。\n",
    "\n",
    "BP算法十分重要，再归纳一下：对每个训练实例，BP算法先做一次预测（前向传播），然后计算误差，然后反向通过每一层以测量误差贡献量（反向传播），最后调整所有连接权重以降低误差（梯度下降）。（译者注：我也总结下吧，每次训练都先是要设置周期epoch数，每次epoch其实做的就是三件事，向前传一次，向后传一次，然后调整参数，接着再进行下一次epoch。）\n",
    "\n",
    "## 激活函数\n",
    "\n",
    "为了使BP算法正常工作，作者对 MLP 的架构做了一个关键调整：用Logistic函数（sigmoid）代替阶跃函数，`σ(z) = 1 / (1 + exp(–z))`。这是必要的，因为阶跃函数只包含平坦的段，因此没有梯度（梯度下降不能在平面上移动），而 Logistic函数处处都有一个定义良好的非零导数，允许梯度下降在每步上取得一些进展。\n",
    "\n",
    "双曲正切函数： `tanh (z) = 2σ(2z) – 1`\n",
    "\n",
    "ReLU 函数：`ReLU(z) = max(0, z)`\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98y3zoo6xj30xc0bgmzs.jpg)\n",
    "\n",
    "## MLP典型架构\n",
    "\n",
    "回归MLP的典型架构。\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98y5e9ac0j30xc0jdjuo.jpg)\n",
    "\n",
    "分类的MLP的典型架构。\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98y63mtkxj30xc0e0ac4.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet\n",
    "\n",
    "http://www.image-net.org\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98yhk3id2j30h9079gm9.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码见code/mlp_keras.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py3.7",
   "language": "python",
   "name": "venv_py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

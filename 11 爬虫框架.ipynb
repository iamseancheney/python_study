{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy爬虫框架\n",
    "\n",
    "各种语言的爬虫框架：https://www.jianshu.com/p/7522e1fc3fb9\n",
    "\n",
    "Scrapy的数据流和组件\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8vag0e607j317a0rk78n.jpg)\n",
    "\n",
    "### scrapy命令行\n",
    "\n",
    "1.创建一个新的项目\n",
    "scrapy startproject [项目名]\n",
    "\n",
    "2.生成爬虫\n",
    "scrapy genspider +文件名+网址\n",
    "\n",
    "3.运行(crawl)\n",
    "scrapy crawl +爬虫名称\n",
    "scrapy crawl [爬虫名] -o zufang.json\n",
    "scrapy crawl [爬虫名] -o zufang.csv\n",
    "\n",
    "4.check检查错误\n",
    "scrapy check\n",
    "\n",
    "5.list返回项目所有spider名称\n",
    "scrapy list\n",
    "\n",
    "6. view 存储、打开网页\n",
    "scrapy view https://www.baidu.com\n",
    "\n",
    "7. scrapy shell，进入终端\n",
    "scrapy shell https://www.baidu.com\n",
    "\n",
    "8. scrapy runspider\n",
    "scrapy runspider zufang_spider.py\n",
    "\n",
    "### 下载器中间件\n",
    "\n",
    "下载器中间件按照优先级被调用的：当request从引擎向下载器传递时，数字小的下载器中间件先执行，数字大的后执行；当下载器将response向引擎传递，数字大的下载器中间件先执行，小的后执行。\n",
    "\n",
    "scrapy提供了一套基本的下载器中间件，\n",
    "\n",
    "{\n",
    "    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,\n",
    "    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,\n",
    "    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,\n",
    "    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,\n",
    "    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500,\n",
    "    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,\n",
    "    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,\n",
    "    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,\n",
    "    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,\n",
    "    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,\n",
    "    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,\n",
    "    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,\n",
    "    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,\n",
    "    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,\n",
    "}\n",
    "\n",
    "见链接\n",
    "https://docs.scrapy.org/en/latest/topics/settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE\n",
    "\n",
    "下载器中间件是个类，类里可以定义方法，例如process_request()，process_response()，process_exception()\n",
    "\n",
    "process_request():\n",
    "\n",
    "process_request()的参数是request, spider\n",
    "\n",
    "参数request是个字典，字典里包含了headers、url等信息\n",
    "\n",
    "process_request()可以利用参数request里面的信息，对请求做修改，这时一般返回的是None，典型的任务是修改User-agent、变换代理\n",
    "\n",
    "如果根据参数request里的url直接就去做抓取，返回response对象，返回的response对象就会不经过剩下的下载器中间件，直接返回到引擎\n",
    "\n",
    "如果对请求做了修改，返回的是request对象，就会发回到调度器，等待调度\n",
    "\n",
    "process_response(request, response, spider)\n",
    "\n",
    "返回的必须是Response、Request或IgnoreRequest异常\n",
    "\n",
    "\n",
    "### 爬虫中间件\n",
    "\n",
    "爬虫中间件的作用：\n",
    "处理引擎传递给爬虫的响应；\n",
    "处理爬虫传递给引擎的请求；\n",
    "处理爬虫传递给引擎的数据项。\n",
    "\n",
    "scrapy提供的基本爬虫中间件\n",
    "https://docs.scrapy.org/en/latest/topics/settings.html#std:setting-SPIDER_MIDDLEWARES_BASE\n",
    "\n",
    "如何自定义爬虫中间件\n",
    "https://docs.scrapy.org/en/latest/topics/spider-middleware.html#writing-your-own-spider-middleware\n",
    "\n",
    "### 管道\n",
    "\n",
    "每个管道组件都是一个实现了某个功能的Python类，常见功能有：\n",
    "清理html数据\n",
    "做确认\n",
    "查重\n",
    "存入数据库\n",
    "\n",
    "每个管道组件的类，必须要有以下方法：\n",
    "process_item(self, item, spider)\n",
    "open_spider(self, spider)\n",
    "close_spider(self, spider)\n",
    "from_crawler(cls, crawler)\n",
    "\n",
    "\n",
    "#### 丢弃数据项\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class PricePipeline(object):\n",
    "\n",
    "    vat_factor = 1.15\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if item['price']:\n",
    "            if item['price_excludes_vat']:\n",
    "                item['price'] = item['price'] * self.vat_factor\n",
    "            return item\n",
    "        else:\n",
    "            raise DropItem(\"Missing price in %s\" % item)\n",
    "\n",
    "\n",
    "#### 存储到MongoDB\n",
    "\n",
    "import pymongo\n",
    "\n",
    "class MongoPipeline(object):\n",
    "\n",
    "    collection_name = 'scrapy_items'\n",
    "\n",
    "    def __init__(self, mongo_uri, mongo_db):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(\n",
    "            mongo_uri=crawler.settings.get('MONGO_URI'),\n",
    "            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')\n",
    "        )\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.db[self.collection_name].insert_one(dict(item))\n",
    "        return item\n",
    "\n",
    "\n",
    "#### 存储到MySQL\n",
    "class MysqlPipeline():\n",
    "    def __init__(self, host, database, user, password, port):\n",
    "        self.host = host\n",
    "        self.database = database\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.port = port\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(\n",
    "            host=crawler.settings.get('MYSQL_HOST'),\n",
    "            database=crawler.settings.get('MYSQL_DATABASE'),\n",
    "            user=crawler.settings.get('MYSQL_USER'),\n",
    "            password=crawler.settings.get('MYSQL_PASSWORD'),\n",
    "            port=crawler.settings.get('MYSQL_PORT'),\n",
    "        )\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.db = pymysql.connect(self.host, self.user, self.password, self.database, charset='utf8',\n",
    "                                  port=self.port)\n",
    "        self.cursor = self.db.cursor()\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.db.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        print(item['title'])\n",
    "        data = dict(item)\n",
    "        keys = ', '.join(data.keys())\n",
    "        values = ', '.join(['%s'] * len(data))\n",
    "        sql = 'insert into %s (%s) values (%s)' % (item.table, keys, values)\n",
    "        self.cursor.execute(sql, tuple(data.values()))\n",
    "        self.db.commit()\n",
    "        return item\n",
    "\n",
    "#### 去重\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class DuplicatesPipeline(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ids_seen = set()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if item['id'] in self.ids_seen:\n",
    "            raise DropItem(\"Duplicate item found: %s\" % item)\n",
    "        else:\n",
    "            self.ids_seen.add(item['id'])\n",
    "            return item\n",
    "\n",
    "#### 激活管道\n",
    "ITEM_PIPELINES = {\n",
    "    'myproject.pipelines.PricePipeline': 300,\n",
    "    'myproject.pipelines.JsonWriterPipeline': 800,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码见code/scrapy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conda 管理虚拟环境\n",
    "\n",
    "创建虚拟环境\n",
    "conda create -n env_name python=3.7\n",
    "\n",
    "删除虚拟环境\n",
    "conda remove -n your_env_name --all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py3.7",
   "language": "python",
   "name": "venv_py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
